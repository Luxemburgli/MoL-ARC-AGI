{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo running the Llama-3.2-3B-ARChitects-ReArc-bnb-4bit model \n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "1. The `Llama-3.2-3B-ARChitects-ReArc-bnb-4bit` folder should be at the top level of this project and is imported as a git submodule when your ran `git submodule update --init`\n",
    "2. `Llama-3.2-3B-ARChitects-ReArc-bnb-4bit/model.safetensors` is a large file and is pulled not from github.com but huggingface.com. This means that you need to either setup an (ssh key)[https://huggingface.co/docs/hub/security-git-ssh] or personal access token for your huggingface account. Once this is done you can pull `the model.safetensors` file by `cd Llama-3.2-3B-ARChitects-ReArc-bnb-4bit` followed by `git lfs pull`. Note git LFS should be  installed and initialised as per the *Getting Started* section in the main read me.\n",
    "3. All the required packages (except for `bitsandbytes non CUDA backend` ) are managed by uv. Run `uv sync` to make sure they are installed.\n",
    "4. Manually install `bitsandbytes non CUDA backend` with this (guide)[https://huggingface.co/docs/bitsandbytes/main/en/installation?backend=Intel+CPU+%2B+GPU#multi-backend] by huggingface. Availability is hardware dependant, I suspect the mac users among us do not have their hardware supported - in this case we can move this repository onto a hosted platform with cloud compute.\n",
    "\n",
    "Notes/recommendations:\n",
    "\n",
    "If you are compiling the `bitsandbytes`package with a non-CUDA backend from source. Clone the repo adjacent to this one and follow the build instructions. You can install the package to the venv associated with this repo via running `uv pip install -e \"../bitsandbytes/\"`in your terminal.\n",
    "\n",
    "In general you can prefix any `pip install` command with uv for the uv package manager to add packages installed this way to its dependency graph.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/natasha/code/MoL-ARC-AGI/Llama-3.2-3B-ARChitects-ReArc-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# path to repository cloned as a submodule\n",
    "model_repo_path = Path('../../Llama-3.2-3B-ARChitects-ReArc-bnb-4bit')\n",
    "\n",
    "#check directory is as expected\n",
    "print(model_repo_path.resolve())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
